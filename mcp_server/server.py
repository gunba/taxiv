"""
MCP server for Taxiv unified search & markdown export.
Exposes LLM tools via FastMCP (SSE transport).

All tool outputs default to **markdown**. JSON output is available via `format="json"` when useful.
"""

from __future__ import annotations
import os
import json
import textwrap
from typing import Any, Dict, List, Optional

import httpx
from fastmcp import FastMCP

BACKEND_BASE_URL = os.environ.get("BACKEND_BASE_URL", "http://backend:8000")
ACT_ID_DEFAULT = os.environ.get("ACT_ID_DEFAULT", "ITAA1997")
REQUEST_TIMEOUT = float(os.environ.get("MCP_HTTP_TIMEOUT", "30"))

INSTRUCTIONS = """
You are connected to the Taxiv MCP server.

**What you can do:**
- Use `unified_search` for **one-bar** queries that mix node refs (e.g., "s 355-100", "Div 152") and terms (e.g., "feedstock").
- Use `lookup_ref` when you already have a canonical ref like `ITAA1997:Section:355-100` and need the internal id.
- Use `fetch_markdown` to retrieve **canonical markdown** (single node or subtree) generated by the backend’s export library.
- Use `get_breadcrumbs` to fetch the hierarchy path for context.

**Best practices:**
- Prefer explicit section or division identifiers when known (e.g., "s 6-5", "Div 152") and combine with terms (e.g., "active asset").
- Request small `k` (e.g., 10–25) first; then drill down with `fetch_markdown` on specific results.
- URS scores (0–100) are **query-independent**; 50 is neutral, >70 is strong relevance.
- Use `lookup_ref` + `fetch_markdown` for precise targets.

All tool responses are in markdown by default. If you need raw data for programmatic reasoning, pass `format="json"`.
"""

def mmd_escape(text: str) -> str:
	"""Light escape for markdown text in inline contexts."""
	return text.replace("<", "&lt;").replace(">", "&gt;").strip()


class Backend:
	def __init__(self, base_url: str, timeout: float = 30):
		self.base_url = base_url.rstrip("/")
		self.client = httpx.AsyncClient(timeout=timeout)

	async def close(self):
		await self.client.aclose()

	async def unified_search(self, query: str, k: int = 25, include_explanations: bool = True) -> Dict[str, Any]:
		url = f"{self.base_url}/api/search/unified"
		payload = {"query": query, "k": k, "include_explanations": include_explanations}
		r = await self.client.post(url, json=payload)
		r.raise_for_status()
		return r.json()

	async def lookup_ref(self, ref_id: str, act_id: str) -> List[Dict[str, Any]]:
		# Returns ProvisionDetail[]; we will echo the first item prominently in markdown
		url = f"{self.base_url}/api/provisions/lookup"
		r = await self.client.get(url, params={"ref_id": ref_id, "act_id": act_id})
		if r.status_code == 404:
			return []
		r.raise_for_status()
		return r.json()

	async def get_detail(self, internal_id: str) -> Dict[str, Any]:
		url = f"{self.base_url}/api/provisions/detail/{internal_id}"
		r = await self.client.get(url)
		r.raise_for_status()
		return r.json()

	async def fetch_markdown(self, internal_id: str, include_descendants: bool) -> str:
		url = f"{self.base_url}/api/provisions/export_markdown"
		r = await self.client.post(url, json={
			"provision_internal_id": internal_id,
			"include_descendants": include_descendants
		})
		r.raise_for_status()
		data = r.json()
		return data.get("markdown", "")

	async def get_breadcrumbs(self, internal_id: str) -> List[Dict[str, str]]:
		url = f"{self.base_url}/api/provisions/breadcrumbs/{internal_id}"
		r = await self.client.get(url)
		r.raise_for_status()
		return r.json()


def format_search_results_md(payload: Dict[str, Any]) -> str:
	qi = payload.get("query_interpretation", {})
	results = payload.get("results", [])
	dbg = payload.get("debug", {}) or {}

	parts: List[str] = []
	# Query interpretation
	parts.append("### Query interpretation")
	lines = []
	provs = qi.get("provisions", [])
	defs = qi.get("definitions", [])
	kws = qi.get("keywords", "")
	if provs:
		lines.append(f"- provisions: `{', '.join(provs)}`")
	if defs:
		lines.append(f"- definitions: `{', '.join(defs)}`")
	if kws:
		lines.append(f"- keywords: `{mmd_escape(kws)}`")
	if not lines:
		lines.append("- *(no structured seeds; likely free-text)*")
	parts.append("\n".join(lines))

	# Results
	parts.append("\n### Top results")
	if not results:
		parts.append("> No results.")
	else:
		for i, item in enumerate(results, start=1):
			title = mmd_escape(item.get("title", ""))
			ref_id = item.get("ref_id", "")
			iid = item.get("id", "")
			typ = item.get("type", "")
			urs = item.get("score_urs", 0)
			parts.append(f"**{i}. {title}**  \n`{ref_id}` — *{typ}* — **URS {urs}**  \n`internal_id: {iid}`")
			# Why
			why = item.get("why", []) or []
			if why:
				parts.append("  - *Why:*")
				for w in why[:3]:
					parts.append(f"    - {w.get('type')}: {mmd_escape(w.get('detail',''))}")
			# Snippet
			snip = item.get("snippet")
			if snip:
				parts.append(f"\n> {mmd_escape(snip)}\n")
	# Debug
	parts.append("\n### Debug")
	parts.append(f"- mass_captured: `{dbg.get('mass_captured', 0)}`")
	parts.append(f"- num_seeds: `{dbg.get('num_seeds', 0)}`")

	# Hints
	parts.append(textwrap.dedent("""
	---
	**Next steps**
	- Use `lookup_ref` if you have a canonical `ACT:Type:ID`.
	- Use `fetch_markdown` with an `internal_id` from above for full text (and definitions/references).
	""").strip())

	return "\n".join(parts).strip()


def create_server() -> FastMCP:
	mcp = FastMCP(name="Taxiv MCP", instructions=INSTRUCTIONS)
	be = Backend(BACKEND_BASE_URL, timeout=REQUEST_TIMEOUT)

	@mcp.on_shutdown
	async def _shutdown():
		await be.close()

	@mcp.tool()
	async def help() -> str:
		"""
		Show best-practice guidance for using this MCP server.
		"""
		return INSTRUCTIONS.strip()

	@mcp.tool()
	async def unified_search(query: str, k: int = 25, include_explanations: bool = True, format: str = "markdown") -> str:
		"""
		Unified relatedness search (nodes + terms + free-text in one query).
		Default output is markdown. Pass format="json" for raw payload.

		Args:
			query: The query string (e.g., "s 355-100 + feedstock", "Div 152 active asset", "ordinary income").
			k: Number of results (typ. 10–25).
			include_explanations: Include "why" reasons.
			format: "markdown" | "json"
		"""
		if not query or not query.strip():
			return "_Empty query supplied._"

		payload = await be.unified_search(query=query, k=min(max(int(k), 1), 100), include_explanations=bool(include_explanations))
		if format.lower() == "json":
			return "```json\n" + json.dumps(payload, indent=2) + "\n```"
		return format_search_results_md(payload)

	@mcp.tool()
	async def lookup_ref(ref_id: str, act_id: str = ACT_ID_DEFAULT, format: str = "markdown") -> str:
		"""
		Resolve a canonical reference (e.g., ITAA1997:Section:355-100) to provision detail(s).
		Returns the first result prominently and lists others if present.
		"""
		items = await be.lookup_ref(ref_id=ref_id, act_id=act_id)
		if not items:
			return f"_No provision found for `{ref_id}` in `{act_id}`._"

		parts = [f"### Lookup `{ref_id}` in `{act_id}`\n"]
		for i, d in enumerate(items, start=1):
			title = mmd_escape(d.get("title", ""))
			ref = d.get("ref_id", "")
			iid = d.get("internal_id", "")
			typ = d.get("type", "")
			parts.append(f"**{i}. {title}**  \n`{ref}` — *{typ}*  \n`internal_id: {iid}`")
		if format.lower() == "json":
			return "```json\n" + json.dumps(items, indent=2) + "\n```"
		return "\n\n".join(parts)

	@mcp.tool()
	async def fetch_markdown(internal_id: str, include_descendants: bool = False, max_chars: Optional[int] = None) -> str:
		"""
		Export canonical markdown for a provision (and optionally its subtree).
		This uses the backend's export_markdown utility (no separate renderer here).

		Args:
			internal_id: Provision internal id.
			include_descendants: If true, exports the subtree under the node.
			max_chars: Optional safety cap on returned markdown size.
		"""
		md = await be.fetch_markdown(internal_id=internal_id, include_descendants=bool(include_descendants))
		if max_chars and isinstance(max_chars, int) and max_chars > 0 and len(md) > max_chars:
			md = md[:max_chars].rstrip() + "\n\n… *(truncated)*"
		return md

	@mcp.tool()
	async def get_breadcrumbs(internal_id: str, format: str = "markdown") -> str:
		"""
		Return the breadcrumb path for a provision, as markdown list.
		"""
		crumbs = await be.get_breadcrumbs(internal_id=internal_id)
		if not crumbs:
			return f"_No breadcrumbs found for `{internal_id}`._"
		if format.lower() == "json":
			return "```json\n" + json.dumps(crumbs, indent=2) + "\n```"

		parts = [f"### Breadcrumbs for `{internal_id}`", ""]
		for c in crumbs:
			parts.append(f"- {mmd_escape(c.get('title',''))}  \n  `internal_id: {c.get('internal_id','')}`")
		return "\n".join(parts)

	return mcp


def main():
	server = create_server()
	host = os.environ.get("MCP_HOST", "0.0.0.0")
	port = int(os.environ.get("MCP_PORT", "8765"))
	# SSE transport is the default recommended mode for ChatGPT / connectors.
	server.run(transport="sse", host=host, port=port)


if __name__ == "__main__":
	main()
